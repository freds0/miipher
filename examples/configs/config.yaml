preprocess:
  preprocess_dataset:
    _target_: torch.utils.data.ConcatDataset
    datasets:
      - 
        _target_: miipher.dataset.libritts.LibriTTSCorpus
        root: /mnt/DATASETS/LibriTTS_R_samples/train-clean-100/
      # - 
      #   _target_: miipher.dataset.jvs_corpus.JVSCorpus
      #   root: /mnt/hdd/datasets/jvs_ver1/
      #- 
      #  _target_: miipher.dataset.libritts.LibriTTSCorpus
      #   root: /mnt/hdd/datasets/libritts-r/LibriTTS_R/  
  phoneme_tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: "vinai/xphonebert-base"
  text2phone_model:
    _target_: text2phonemesequence.Text2PhonemeSequence
    is_cuda: True
  degration:
    format_encoding_pairs:
    reverb_conditions:
      p: 0.5
      reverbation_times:
        max: 0.5
        min: 0.2
      room_xy:
        max: 10.0
        min: 2.0
      room_z:
        max: 5.0
        min: 2.0
      room_params:
        fs: 22050
        max_order: 10
        absorption: 0.2
      source_pos:
        - 1.0
        - 1.0
        - 1.0
      mic_pos:
        - 1.0
        - 0.7
        - 1.2
    n_rirs: 10
    background_noise:
      snr:
        max: 30.0
        min: 5.0
      patterns:
        - 
          - /mnt/DATASETS/TAU_urban/audio/
          - '*.wav'
        # - 
        #   # - /mnt/hdd/datasets/TAU_urban/audio/
        #   - /data/hy17/noise/DNS
        #   - '**/*.wav'
  train_tar_sink:
    _target_: webdataset.ShardWriter
    pattern: "/mnt/miipher/miipher-train-%06d.tar.gz"
  val_tar_sink:
    _target_: webdataset.ShardWriter
    pattern: "/mnt/miipher/miipher-val-%06d.tar.gz"
  val_size: 10
  n_repeats: 4
sample_rate: 22050

data:
  train_dataset_path: /mnt/miipher/miipher-train-{000000..0000}.tar.gz
  val_dataset_path: /mnt/miipher/miipher-val-{000000..000000}.tar.gz
  train_batch_size: 1
  val_batch_size: 1
  speech_ssl_processor:
    # --- wavlm --- 
    processor: 
      _target_: transformers.AutoFeatureExtractor.from_pretrained
      pretrained_model_name_or_path: "microsoft/wavlm-large"
    sr: 16_000
     # --- w2v-bert ---
    #processor:
    #  _target_: transformers.AutoFeatureExtractor.from_pretrained
    #  pretrained_model_name_or_path: "facebook/w2v-bert-2.0"
    #sr: 16_000
  phoneme_padding_idx: 1
  phoneme_tokenizer:
    _target_: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: "vinai/xphonebert-base"
train:
  loggers:
    # - _target_: lightning.pytorch.loggers.WandbLogger
    #   project: "miipher"
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: "./tb_runs/wavlm-bert"
      #save_dir: "./tb_runs/w2v-bert"
      # project: "miipher-w2v-bert" # **** TODO(HAICI) ***
  trainer:
    _target_: lightning.Trainer
    accelerator: "gpu"
    devices: -1 # **** TODO(HAICI) ***
    check_val_every_n_epoch: 1
    max_epochs: 3300
model:
  ssl_models: # **** TODO(HAICI) ***
    # --- WavLM ---
    model:
      _target_: transformers.AutoModel.from_pretrained
      pretrained_model_name_or_path: "microsoft/wavlm-large"
    # # --- Whisper ---
    # model:
    #   _target_: transformers.AutoModel.from_pretrained
    #   pretrained_model_name_or_path: "openai/whisper-large-v2"
    # # --- W2v-bert ---
    #model:
      #_target_: transformers.Wav2Vec2BertModel.from_pretrained
      #pretrained_model_name_or_path: "facebook/w2v-bert-2.0"
    sr: 16_000
    layer: 8  

  phoneme_model:
    _target_: transformers.AutoModel.from_pretrained
    pretrained_model_name_or_path: "vinai/xphonebert-base"
  xvector_model:
    _target_: speechbrain.pretrained.EncoderClassifier.from_hparams
    source: speechbrain/spkrec-ecapa-voxceleb
  miipher:
    n_phone_feature: 768
    n_speaker_embedding: 192
    n_ssl_feature: 1024
    n_hidden_dim: 1024
    n_conformer_blocks: 4
    n_iters: 2
optimizers:
  _target_: torch.optim.AdamW
  lr: 2e-5

